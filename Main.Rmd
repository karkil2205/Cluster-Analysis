	---
title: "Cluster Analysis"
output:
  html_notebook:
    toc: yes
  word_document:
    toc: yes
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    df_print: paged
---




<img src="https://upload.wikimedia.org/wikipedia/commons/6/66/Logo_cnam.gif" width="200">
 


* Required packages

```{r```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
#install.packages("dplyr")
#install.packages("stargazer")
#install.packages("ade4")
#install.packages("magrittr")
#install.packages("cluster")
#install.packages("factoextra")
#install.packages("cluster.datasets"



}
knitr::opts_chunk$set(echo = TRUE)
```



# Definition of a distance

* A distance function or a metric on $\mathbb{R}^n,\:n\geq 1$, is a function $d:\mathbb{R}^n\times\mathbb{R}^n\rightarrow \mathbb{R}$.
* A distance function must satisfy some required properties or axioms. 
* There are three main axioms.
* A1. $d(\mathbf{x},\mathbf{y})= 0\iff \mathbf{x}=\mathbf{y}$ (identity of indiscernibles);
* A2. $d(\mathbf{x},\mathbf{y})= d(\mathbf{y},\mathbf{x})$ (symmetry);

* A3. $d(\mathbf{x},\mathbf{z})\leq d(\mathbf{x},\mathbf{y})+d(\mathbf{y},\mathbf{z})$  (triangle inequality),
where $\mathbf{x}=(x_1,\cdots,x_n)$, $\mathbf{y}=(y_1,\cdots,y_n)$ and $\mathbf{z}=(z_1,\cdots,z_n)$ are all vectors of $\mathbb{R}^n$.
* We should use the term _dissimilarity_ rather than _distance_ when not all the three axioms A1-A3 are valid.
* Most of the time, we shall use, with some abuse of vocabulary, the term distance.



# Exercice 1

* Prove that the three axioms A1-A3 imply the non-negativity condition: $$d(\mathbf{x},\mathbf{y})\geq 0.$$



# Euclidean distance

* It is defined by:

$$
d(\mathbf{x},\mathbf{y})=\sqrt{\sum_{i=1}^n (x_i-y_i)^2}.
$$



* A1-A2 are obvious.
* The proof of A3 is provided below.



# Exercice 2

* Is the squared Euclidian distance a true distance?



# Manhattan distance

* The Manhattan distance also called  taxi-cab metric or city-block metric is defined by:

$$d(\mathbf{x},\mathbf{y})
=\sum_{i=1}^n |x_i-y_i|.$$

* A1-A2 hold.
* A3 also holds using the fact that $|a+b|\leq |a|+|b|$ for any reals $a,b$.
* There exists also a weighted version  of the Manhattan distance called the Canberra distance.



![](Manhattan.jpg)




```{r}
x = c(0, 0)
y = c(6,6)
dist(rbind(x, y), method = "euclidian")
dist(rbind(x, y), method = "euclidian",diag=T,upper=T)

6*sqrt(2)
dist(rbind(x, y), method = "manhattan")
dist(rbind(x, y), method = "manhattan",diag=T,upper=T)

```





# Canberra distance

* It is defined by:

$$d(\mathbf{x},\mathbf{y})
=\sum_{i=1}^n
 \frac{|x_i-y_i|}{|x_i|+|y_i|}.$$

* Note that the term $|x_i-y_i|/(|x_i|+|y_i|)$ is not properly defined as: $x_i=y_i=0$.
* By convention we set that term to be zero in that case.
* The Canberra distance is specially sensitive to small changes near zero.


```{r}
x = c(0, 0)
y = c(6,6)
dist(rbind(x, y), method = "canberra")
6/6+6/6
```




# Exercice 3

* Prove that the Canberra distance is a true distance, i.e. that it satisfies A1-A3.





# Minkowski distance
* Both the Euclidian and the Manattan distances are special cases of  the Minkowski distance which is defined, for $p\geq 1$, by: 
$$
d(\mathbf{x},\mathbf{y})=
\left[\sum_{i=1} |x_i-y_i|^{p}\right]^{1/p}.
$$
 * For $p=1$, we get the Manhattan distance.
 * For $p=2$, we get the Euclidian distance.
* Let us also define: 
$$\|\mathbf{x}\|_p\equiv\left[\sum_{i=1}^n |x_i|^{p}\right]^{1/p},$$
where $\|\mathbf{\cdot}\|_p$ is known as the $p$-norm or Minkowski norm.
* Note that the Minkowski distance and norm are related by:

$$d(\mathbf{x},\mathbf{y})=\|\mathbf{x}-\mathbf{y}\|_p.$$

* Conversely, we have:

$$\|\mathbf{x}\|_p=d(\mathbf{x},\mathbf{0}),$$

where 
$\mathbf{0}$ is the null-vetor of $\mathbb{R}^n$.





```{r}
library("ggplot2")
x = c(0, 0)
y = c(6,6)
MinkowDist=c() # Initialiser à vide la liste
for (p in seq(1,30,.01))
{
MinkowDist=c(MinkowDist,dist(rbind(x, y), method = "minkowski", p = p))     
}

ggplot(data =data.frame(x = seq(1,30,.01), y=MinkowDist ) , mapping = aes( x=x, y= y))+
  geom_point(size=.1,color="red")+
  xlab("p")+ylab("Minkowski Distance")+ggtitle("Minkowski distance wrt p")
```



# Exercice 4

Produce a similar graph using "The Economist" theme. Indicate on the graph the Manhattan, the Euclidian distances as well as the Chebyshev distance introduces below.



# Chebyshev distance 


* At the limit, we get the Chebyshev distance which is defined by:
$$
d(\mathbf{x},\mathbf{y})=\max_{i=1,\cdots,n}(|x_i-y_i|)=\lim_{p\rightarrow\infty}
\left[\sum_{i=1} |x_i-y_i|^{p}\right]^{1/p}.
$$

* The corresponding norm is:

$$
\|\mathbf{x}\|_\infty=\max_{i=1,\cdots,n}(|x_i|).
$$




# Minkowski inequality

* The proof of the triangular inequality A3 is based on the Minkowski inequality:
* For any nonnegative real numbers $a_1,\cdots,a_n$; $b_1,\cdots,b_n$, and for any $p\geq 1$, we have:
$$
\left[\sum_{i=1}^n (a_i+b_i)^{p}\right]^{1/p}\leq
\left[\sum_{i=1}^n a_i^{p}\right]^{1/p}
+\left[\sum_{i=1}^n b_i^{p}\right]^{1/p}.
$$
* To prove that the Minkowski distance satisfies A3, notice that 
$$
 \sum_{i=1}^n|x_i-z_i|^{p}= \sum_{i=1}^n|(x_i-y_i)+(y_i-z_i)|^{p}.
$$
* Since for any reals $x,y$, we have: $|x+y|\leq |x|+|y|$, and using the fact that $x^p$ is increasing in $x\geq 0$, we obtain:
$$
 \sum_{i=1}^n|x_i-z_i|^{p}\leq \sum_{i=1}^n(|x_i-y_i|+|y_i-z_i|)^{p}.
$$

* Applying the Minkowski inequality with $a_i=|x_i-y_i|$ and $b_i=|y_i-z_i|$, $i=1,\cdots,n$, we get:
$$
 \sum_{i=1}^n|x_i-z_i|^{p}\leq \left(\sum_{i=1}^n |x_i-y_i|^{p}\right)^{1/p}+\left(\sum_{i=1}^n |y_i-z_i|^{p}\right)^{1/p}.
$$


# Exercice 5
To illustarate the Minkowski inequality, draw $100$ times two lists of $100$ draws from the lognormal distribution with mean $1600$ and standard-deviation $300$. Illustrate with a graph the gap between the two drawn lists.

# Hölder inequality

* The proof of the Minkowski inequality itself requires the Hölder inequality:
* For any nonnegative real numbers $a_1,\cdots,a_n$; $b_1,\cdots,b_n$, and any $p,q>1$ with $1/p+1/q=1$, we have:
$$
\sum_{i=1}^n a_ib_i\leq
\left[\sum_{i=1}^n a_i^{p}\right]^{1/p}
\left[\sum_{i=1}^n b_i^{q}\right]^{1/q}
$$
* The proof of the Hölder inequality relies on the Young inequality:
* For any $a,b>0$, we have
$$
ab\leq \frac{a^p}{p}+\frac{b^q}{q},
$$
with equality occuring iff: $a^p=b^q$. 
* To prove the Young inequality, one can use the (strict) convexity of the exponential function.
* For any reals $x,y$, we have:
$$
e^{\frac{x}{p}+\frac{y}{q} }\leq \frac{e^{x}}{p}+\frac{e^{y}}{q}. 
$$
* We then set: $x=p\ln a$ and $y=q\ln b$ to get the Young inequality.
* A good reference on inequalities is: Z. Cvetkovski,  Inequalities: theorems, techniques and selected problems, 2012, Springer Science & Business Media.
 # Cauchy-Schwartz inequality
* Note that the triangular inequality for the Minkowski distance implies: 
$$
\sum_{i=1}^n |x_i|\leq
\left[\sum_{i=1}^n |x_i|^{p}\right]^{1/p}.
$$
* Note that for $p=2$, we have $q=2$. The Hölder inequality implies for that special case
$$
\sum_{i=1}^n|x_iy_i|\leq\sqrt{\sum_{i=1}^n x_i^2}\sqrt{\sum_{i=1}^n y_i^2}. 
$$
* Since the LHS od thes above inequality is greater then $|\sum_{i=1}^nx_iy_i|$, we get the Cauchy-Schwartz inequality

$$
|\sum_{i=1}^nx_iy_i|\leq\sqrt{\sum_{i=1}^n x_i^2}\sqrt{\sum_{i=1}^n y_i^2}. 
$$
* Using the dot product notation called also scalar product noation: $\mathbf{x\cdot y}=\sum_{i=1}^nx_iy_i$, and the norm notation $\|\mathbf{\cdot}\|_2 \|$, the Cauchy-Schwart inequality is:
$$
|\mathbf{x\cdot y} | \leq \|\mathbf{x}\|_2 \| \mathbf{y}\|_2.
$$




# Pearson correlation distance 

* The Pearson correlation coefficient is a similarity measure on $\mathbb{R}^n$ defined by:
$$
\rho(\mathbf{x},\mathbf{y})=
\frac{\sum_{i=1}^n (x_i-\bar{\mathbf{x}})(y_i-\bar{\mathbf{y}})}{{\sqrt{\sum_{i=1}^n (x_i-\bar{\mathbf{x}})^2\sum_{i=1}^n (y_i-\bar{\mathbf{y}})^2}}},
$$
where $\bar{\mathbf{x}}$ is the mean of the vector $\mathbf{x}$ defined by: 
$$\bar{\mathbf{x}}=\frac{1}{n}\sum_{i=1}^n x_i,$$
* Note that the Pearson correlation coefficient satisfies P2 and  is invariant to any positive linear transformation, i.e.: $$\rho(\alpha\mathbf{x},\mathbf{y})=\rho(\mathbf{x},\mathbf{y}),$$ for any $\alpha>0$.
* The Pearson distance (or correlation distance) is defined by:
$$
d(\mathbf{x},\mathbf{y})=1-\rho(\mathbf{x},\mathbf{y}).
$$

* Note that the Pearson distance does not satisfy A1 since $d(\mathbf{x},\mathbf{x})=0$ for any non-zero vector $\mathbf{x}$. It neither satisfies the triangle inequality. However, the symmetry property is fullfilled. 



# Cosine correlation distance


* The cosine of the angle $\theta$ between two vectors $\mathbf{x}$ and $\mathbf{y}$ is a measure of similarity given by:
$$
\cos(\theta)=\frac{\mathbf{x}\cdot \mathbf{y}}{\|\mathbf{x}\|_2\|\mathbf{y}\|_2}=\frac{\sum_{i=1}^n x_i y_i}{{\sqrt{\sum_{i=1}^n x_i^2\sum_{i=1}^n y_i^2}}}.
$$
* Note that the cosine of the angle between the two centred vectors $\mathbf{x}-\bar{\mathbf{x}}\mathbf{1}$ and $\mathbf{y}-\bar{\mathbf{y}}\mathbf{1}$ coincides with the Pearson correlation coefficient of $\mathbf{x}$ and $\mathbf{y}$, where $\mathbf{1}$ is a vector of units of $\mathbb{R}^n$.
* The cosine correlation distance is defined by:
$$
d(\mathbf{x},\mathbf{y})=1-\cos(\theta).
$$
* It shares similar properties than the Pearson correlation distance. Likewise, Axioms A1 and A3 are not satisfied.




# Spearman correlation distance 

* To calculate the Spearman's rank-order correlation, we need to map seperately each of the vectors to ranked data values: 
$$\mathbf{x}\rightarrow \text{rank}(\mathbf{x})=(x_1^r,\cdots,x_n^r).$$
* Here, $x_i^r$ is the rank of $x_i$ among the set of values of $\mathbf{x}$.
* We illustrate this transformation with a simple example:
* If $\mathbf{x}=(3, 1, 4, 15, 92)$, then the rank-order vector is $\text{rank}(\mathbf{x})=(2,1,3,4,5)$.  

```{r}
x=c(3, 1, 4, 15, 92)
rank(x)
```

* The Spearman's rank correlation of two numerical variables $\mathbf{x}$  and $\mathbf{y}$ is simply the Pearson correlation of the two correspnding rank-order variables $\text{rank}(\mathbf{x})$ and $\text{rank}(\mathbf{y})$, i.e. $\rho(\text{rank}(\mathbf{x}),\text{rank}(\mathbf{y}))$. This measure is is useful because it is more robust against outliers than the Pearson correlation.
* If all  the $n$  ranks are distinct, it can be computed using the following formula:
$$
\rho(\text{rank}(\mathbf{x}),\text{rank}(\mathbf{y}))=1-\frac{6\sum_{i=1}^n d_i^2}{n(n^2-1)},
$$
where $d_i=x_i^r-y_i^r,\:i=1,\cdots,n$.
 * The spearman distance is then defined by:
$$
d(\mathbf{x},\mathbf{y})=1-\rho(\text{rank}(\mathbf{x}),\text{rank}(\mathbf{y})).
$$
* It can be shown that easaly that it is not a proper distance.
* If all  the $n$  ranks are distinct, we get:
$$
d(\mathbf{x},\mathbf{y})=\frac{6\sum_{i=1}^n d_i^2}{n(n^2-1)}.
$$





```{r}
x=c(3, 1, 4, 15, 92)
rank(x)
y=c(30,2 , 9, 20, 48)
rank(y)
d=rank(x)-rank(y)
d
cor(rank(x),rank(y))
1-6*sum(d^2)/(5*(5^2-1))
```


# Exercice 6
For the two vectors $\mathbf{x}=(22,34,1,12,25,56,7)$ and $\mathbf{y}=(2,64,12,2,22,5,8)$ :

* Calculate the ranks for each vector.
* Deduce the Spearman correlation distance from that ranks.
* Deduce the Spearman correlation distance from the above dispalyed alternative equation.
* Calcualate the Spearman correlation distance using the R function. 



# Kendall tau distance 

* The Kendall rank correlation coefficient is calculated from the number of correspondances between the rankings of $\mathbf{x}$ and the rankings of $\mathbf{y}$.
*   The number of pairs of observations among $n$ observations or values is: 
$${n \choose 2} =\frac{n(n-1)}{2}.$$
* The pairs of observations $(x_{i},x_{j})$  and  $(y_{i},y_{j})$ are said to be _concordant_ if: $$\text{sign}(x_j-x_i)=\text{sign}(y_j-y_i),$$ and to be _discordant_ if:  $$\text{sign}(x_j-x_i)=-\text{sign}(y_j-y_i),$$
where $\text{sign}(\cdot)$ returns  $1$ for positive numbers and  $-1$ negative numbers and $0$ otherwise.
* If $x_i=x_j$ or $y_i=y_j$ (or both), there is a tie.
* The Kendall $\tau$ coefficient is defined by (neglecting ties):
$$\tau =\frac {1}{n(n-1)}\sum_{i=1}^{n}\sum_{j=1}^n\text{sign}(x_j-x_i)\text{sign}(y_j-y_i).$$
* Let $n_c$ (resp. $n_d$) be the number of concordant (resp. discordant) pairs, we have $$\tau =\frac {2(n_c-n_d)}{n(n-1)}.$$ 
* The Kendall tau distance is then: $$d(\mathbf{x},\mathbf{y})=1-\tau. $$
* Remark: the triangular inequality may fail in cases where there are ties.



```{r}
x=c(3, 1, 4, 15, 92)
y=c(30,2 , 9, 20, 48)
tau=0
for (i in 1:5)
{  
tau=tau+sign(x -x[i])%*%sign(y -y[i])
}
tau=tau/(5*4)
tau
cor(x,y, method="kendall")
```

# Exercice 7
For the two vectors $\mathbf{x}=(22,34,1,12,25,56,7)$ and $\mathbf{y}=(2,64,12,2,22,5,8)$ :

* List all pairs of coordinates.
* How many pairs are there?
* For each pair and each cector, compute the signs of the differences in coordinates.
* Deduce the Kendall tau coefficient using the above computations.
* Calcualte the Kendall tau coefficient using the R function. 





# Variables standardization

* Variables are often standardized before measuring dissimilarities.
* Standardization converts the original variables into uniteless variables.
* A well known method is the z-score transformation: 
$$
\mathbf{x}\rightarrow (\frac{x_1-\bar{\mathbf{x}}}{s_\mathbf{x}},\cdots,\frac{x_n-\bar{\mathbf{x}}}{s_\mathbf{x}}),
$$ 
where $s_\mathbf{x}$ is the sample standard deviation given by:
$$
s_\mathbf{x}=\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{\mathbf{x}})^2.
$$
* The transformed variable will have a mean of $0$ and a variance of $1$.
* The result obtained with Pearson correlation measures and standardized Euclidean distances are comparable.
* For other methods, see: Milligan, G. W., & Cooper, M. C. (1988). A study of standardization of variables in cluster analysis. _Journal of classification_, _5_(2), 181-204.




```{r}
x=c(3, 1, 4, 15, 92)
y=c(30,2 , 9, 20, 48)
(x-mean(x))/sd(x)
scale(x)
(y-mean(y))/sd(y)
scale(y)
```



# Similarity measures for binary data
* A common simple situation occurs when all information is of the presence/absence of 2-level qualitative characters.
* We assume there are $n$ characters.
* *The presence of the character is coded by $1$ and the absence by 0. 
* We have have at our disposal two vectors.
* $\mathbf{x}$ is observed for a first individual (or object).
* $\mathbf{y}$ is observed for a second individual. 
* We can then calculate the following four statistics:
 
   $a=\mathbf{x\cdot y}=\sum_{i=1}^nx_iy_i.$
   
   $b=\mathbf{x\cdot (1-y)}=\sum_{i=1}^nx_i(1-y_i).$
   
   $c=\mathbf{(1-x)\cdot y}=\sum_{i=1}^n(1-x_i)y_i.$

   $d=\mathbf{(1-x)\cdot (1-y)}=\sum_{i=1}^n(1-x_i)(1-y_i).$

* The counts of  matches are $a$ for $(1,1)$ and $d$ for $(0,0)$;
* The counts of mismatches are $b$ for $(1,0)$ and $c$ for $(0,1)$.
* Note that obviously: $a+b+c+d= n$.
* This gives a very useful $2 \times 2$ association table. 


|    |    |Second individual|||
|:---|---|:---:|:---:|:---:|
|  |    |1  |0|*Totals*
|**First individual**|1|$a$|$b$|$a+b$|
|             |0|$c$|$d$|$c+d$|
|*Totals*||$a+c$|$b+d$|$n$|





![](KAUFMANBinarydata.jpg)
*Table from Kaufman, L., & Rousseeuw, P. J. (2009). _Finding groups in data: an introduction to cluster analysis_ (Vol. 344). John Wiley & Sons*




* The data shows $8$ people (individuals) and $10$ binary variables:
* Sex, Married, Fair Hair, Blue Eyes, Wears Glasses, Round Face, Pessimist, Evening Type, Is an Only Child, Left-Handed.




```{r}
data=c(
1,0,1,1,0,0,1,0,0,0,
0,1,0,0,1,0,0,0,0,0,
0,0,1,0,0,0,1,0,0,1,
0,1,0,0,0,0,0,1,1,0,
1,1,0,0,1,1,0,1,1,0,
1,1,0,0,1,0,1,1,0,0,
0,0,0,1,0,1,0,0,0,0,
0,0,0,1,0,1,0,0,0,0
)
data=data.frame(matrix(data, nrow=8,byrow=T))
row.names(data)=c("Ilan","Jacqueline","Kim","Lieve","Leon","Peter","Talia","Tina")
names(data)=c("Sex", "Married", "Fair Hair", "Blue Eyes", "Wears Glasses", "Round Face", "Pessimist", "Evening Type", "Is an Only Child", "Left-Handed")
```

* We are comparing the records for Ilan with Talia.
```{r,echo=T,results='asis'}
x=data["Ilan",]
y=data["Talia",]
knitr::kable(table(x, y)[2:1,2:1],"pipe")
```

* Therefore: $a = 1,\:b = 3,\: c = 1,\: d = 5$.
* Note that interchanging Ilan and Talia would permute $b$ and $c$ while leaving $a$ and $d$ unchanged.
* A good similarity or dissimilarity coefficient must treat $b$ and $c$ symmetrically.
* A similarity measure is denoted by: $s(\mathbf{x},\mathbf{y})$.
* The corresponding distance is then defined as: 
 $$d(\mathbf{x},\mathbf{y})=1-s(\mathbf{x},\mathbf{y}).$$
* Alternatively, we have:
$$d(\mathbf{x},\mathbf{y})=\sqrt{1-s(\mathbf{x},\mathbf{y})}.$$ 
* A list of some of the similarity measures $s(\mathbf{x},\mathbf{y})$ that have been suggested for binary data is shown below.
* A more extensive list can be found in: Gower, J. C., & Legendre, P. (1986). Metric and Euclidean properties of dissimilarity coefficients. _Journal of classification_, _3_(1), 5-48.





|Coefficient|$s(\mathbf{x},\mathbf{y})$|$d(\mathbf{x},\mathbf{y})=1-s(\mathbf{x},\mathbf{y})$|
|:---|:---:|:---:|
|Simple matching|$\frac{a+d}{a+b+c+d}$ |$\frac{b+c}{a+b+c+d}$|
|Jaccard|$\frac{a}{a+b+c}$|$\frac{b+c}{a+b+c}$|
|Rogers and Tanimoto (1960)| $\frac{a+d}{a+2(b+c)+d}$|$\frac{2(b+c)}{a+2(b+c)+d}$|
|Gower and Legendre (1986)|$\frac{2(a+d)}{2(a+d)+b+c}$|$\frac{b+c}{2(a+d)+b+c}]$|
|Gower and Legendre (1986)|$\frac{2a}{2a+b+c}$|$\frac{b+c}{2a+b+c}$|

* To calculate these coefficients, we use the function: [dist.binary().](https://www.rdocumentation.org/packages/ade4/versions/1.7-16/topics/dist.binary)

* All the distances  in this package are of type  $d(\mathbf{x}.\mathbf{y})= \sqrt{1 - s(\mathbf{x}.\mathbf{y})}$.

```{r,echo=T,results='asis'}
library(ade4)
a=1
b=3
c=1
d=5
dist.binary(data[c("Ilan","Talia"),],method=2)^2
1-(a+d )/(a+b+c+d)
dist.binary(data[c("Ilan","Talia"),],method=1)^2
1-a/(a+b+c)
dist.binary(data[c("Ilan","Talia"),],method=4)^2
1-(a+d )/(a+2*(b+c)+d)
# One Gower coefficient is missing
dist.binary(data[c("Ilan","Talia"),],method=5)^2
1-2*a/(2*a+b+c)
```

* The reason for such a large number of possible measures has to do with the apparent uncertainty as to how to deal with the count of zero-zero matches $d$.
* The measues embedding $d$ are sometimes called symmetrical.
* The other measues are called assymmetrical.
* In some cases, of course, zero_zero matches are completely equivalent to one–one matches, and therefore should be included in the calculated similarity measure.
* An example is gender, where there is no preference as to which of the two categories should be coded zero or one.
* But in other cases the inclusion or otherwise of $d$ is more problematic; for example, when the zero category corresponds to the genuine absence of some property, such as wings in a study of insects. 

# Exercice 8

* Use the data set *animals* available in the package *cluster*.
* This data set was first used in this textbook [KAUFMAN, Leonard et ROUSSEEUW, Peter J. Finding groups in data: an introduction to cluster analysis. John Wiley & Sons, 2009](https://1lib.fr/book/539806/023f26).


TTT


# Exercice 9

* Prove that the distances based on the  Simple Matching coefficient and the Jaccard coefficient satisfy A3.
* Prove that the distances proposed by Gower and Legendre (1986) do not satisfy A3.
* Hint: Proofs and counterexamples have to be adapted from in the paper: 
* [Gower, J. C., & Legendre, P. (1986). Metric and Euclidean properties of dissimilarity coefficients. _Journal of classification_, _3_(1), 5-48.](https://drive.google.com/file/d/1PUQ7g9HIwwUG0CXbCsLA03hnApWMhjka/view?usp=drivesdk)




# Nominal variables 

* We previuosly studied above binary variables which can only take on two states coded as $0,1$.
* We generalize this approach to nominal variables which may take on more than two states.
* Eye's color may have for example four states: blue, brown, green, grey . 
* Le $M$ be the number of states and code the outcomes as $1, \cdots, M$. 
* We may choose $1 =\text{blue},$ $2 =\text{brown},$ $3 =\text{green},$ and $4 =\text{grey}$.
* These states are not ordered in any way
* One strategy would be creating a new binary variable for each of the $M$ nominal states.
* Then to put it equal to $1$ if the corresponding state occurs and to $0$ otherwise. 
* After that, one could resort to one of the dissimilarity coeffi-
cients of the previous subsection. 
* The most common way of measuring the similarity or dissimilarity between two objects through categorial variables is the simple matching approach. 
* If $\mathbf{x},\mathbf{y},$ are both $n$ nominal records for two individuals,
* Let define the function: 
$$\delta(x_i,y_i)\equiv \begin{cases}0,
\text{ if } x_i=y_i;\\1,\text{ if } x_i \neq y_i.\end{cases}$$

* Let  $N_{a+d}$ be the number of attributes of the two individuals on which the two records match:
$$N_{a+d}=\sum_{i=1}^n\delta(x_i,y_i).$$
* Let $N_{b+c}$ be the number of attributes on which the two records do not match:
$$N_{b+c}= n - N_{a+d}.$$
* Let $N_d$ be the number of attributes on which the two records match in a “not applicable” category:
  $$N_{d}=\sum_{i=1}^n\delta(x_i,y_i).$$

* The distance corresponding to the simple matching approach is: 
$$
d(\mathbf{x},\mathbf{y})=\frac{\sum_{i=1}^n\delta(x_i,y_i)}{n}.
$$
* Therefore:
$$d(\mathbf{x},\mathbf{y})=\frac{N_{a+d}}{N_{a+d}+N_{b+c}}.$$
*  Note that simple matching has exactly the same meaning as in the preceding section.
 
 
 
# Gower’s dissimilarity
* Gower’s coefficient is a dissimilarity measure specifically
designed for handling mixed attribute types or variables.
* See: GOWER, John C. A general coefficient of similarity and some of its properties. _Biometrics_, 1971, p. 857-871.
* The coefficient is calculated as the weighted average of attribute contributions.
* Weights usually used only to indicate which attribute values could actually be compared meaningfully.
* The formula is:
$$
d(\mathbf{x},\mathbf{y})=\frac{\sum_{i=1}^n w_i \delta(x_i,y_i)}{\sum_{i=1}^n w_i}.
$$
* The wheight $w_i$  is put equal to $1$ when both measurements $x_i$ and $y_i$ are nonmissing, 
* The number $\delta(x_i,y_i)$ is the contribution of the $i$th measure or variable to the dissimilarity measure.
* It the $i$th measure is nominal, we take  
$$
\delta(x_i,y_i)\equiv \begin{cases}0,
\text{ if } x_i=y_i;\\1,\text{ if } x_i \neq y_i.\end{cases}
$$
*  If the $i$th measure is  interval-scaled, we take instead: 
$$
\delta(x_i,y_i)\equiv \frac{|x_i-y_i|}{R_i},
$$
where $R_i$ is the range of variable $i$ over the available data.




* Consider the following data set:

![](flowers.jpg)
*Data from: Struyf, A., Hubert, M., & Rousseeuw, P. (1997). Clustering in an object-oriented environment. _Journal of Statistical Software_, _1_(4), 1-30.*




* The dataset contains 18 flowers and 8 characteristics:
1. Winters: binary, indicates whether the plant may be left in the garden when it freezes.
2. Shadow: binary, shows whether the plant needs to stand in the shadow.
3. Tubers (Tubercule): asymmetric binary, distinguishes between plants with tubers and plants that grow in any other way.
4. Color: nominal, specifies the flower’s color (1=white, 2=yellow, 3= pink, 4=red, 5=
blue).
5. Soil: ordinal, indicates whether the plant grows in dry (1), normal (2), or wet (3) soil.
6. Preference: ordinal, someone’s preference ranking, going from 1 to 18.
7. Height: interval scaled, the plant’s height in centimeters.
8. Distance: interval scaled, the distance in centimeters that should be left between the plants.
 * The dissimilarity between Begonia  and Broom (Genêt) can be calculated as follows:
 
![](Begonia.jpg)
*Begonia*

![](Genet.jpeg)
*Genêt*




```{r}
library(cluster)
library(dplyr)
data <-flower %>% 
rename(Winters=V1,Shadow=V2,Tubers=V3,Color=V4,Soil=V5,Preference=V6,Height=V7,Distance=V8) %>%
mutate(Winters=recode(Winters,"1"="Yes","0"="No"),
      Shadow=recode(Shadow,"1"="Yes","0"="No"),
      Tubers=recode(Tubers,"1"="Yes","0"="No"),
      Color=recode(Color,"1"="white", "2"="yellow", "3"= "pink", "4"="red", "5"="blue"),
      Soil=recode(Soil,"1"="dry", "2"="normal", "3"= "wet")
      ) 

```

```{r,result = 'asis'}
res=lapply(data,class)  
res=as.data.frame(res)
res[1,] %>% 
knitr::kable()
```

```{r}
flower[1:2,]
```

```{r}
max(data$Height)-min(data$Height)
max(data$Distance)-min(data$Distance)

```

$$
\frac{|1-0|+|0-1|+|0-1|+1+|1-3|/2+|3-15|/17+|150-25|/180+|50-15|/50}{8}\approx 0.8875408
$$


![](Daisy.jpg)





```{r}
library(cluster)
(abs(1-0)+abs(0-1)+abs(0-1)+1+abs(1-3)/2+abs(3-15)/17+abs(150-25)/180+abs(50-15)/50)/8
daisy(data[,1:8],metric = "Gower") 
```





# More on distance matrix computation

![](USAArrest.jpg)




* We use a subset of the data by taking 15 random rows among the 50 rows in the data set.
* We are using the function sample().
* We standardize the data using the function scale().




```{r}
USArrests
```




```{r}
set.seed(123)
ss <- sample(1:50,15) 
df <- USArrests[ss, ] 
df.scaled <- scale(df) 
df.scaled
```




* The R functions for computing distances.
1. dist() function accepts only numeric data.
2. get_dist() function [factoextra package] accepts only numeric data. it supports correlation-based distance measures.
3. daisy() function [cluster package] is able to handle other variable types (nominal, ordinal, ...). 
* Remark: All these functions compute distances between rows of the data.
* Remark: If we want to compute pairwise distances between variables, we must transpe the data to have variables
in the rows.



* We first compute Euclidian distances

```{r}
dist.eucl <- dist(df.scaled, method = "euclidean")

round(as.matrix(dist.eucl)[1:3, 1:3], 1)

round(sqrt(sum((df.scaled["New Mexico",]-df.scaled["Iowa",])^2)),1)
round(sqrt(sum((df.scaled["New Mexico",]-df.scaled["Indiana",])^2)),1)
round(sqrt(sum((df.scaled["Iowa",]
-df.scaled["Indiana",])^2)),1)


```



* We also compute correlation based distances.

```{r}
library("factoextra")
dist.cor <- get_dist(df.scaled, method = "pearson")
round(as.matrix(dist.cor)[1:3, 1:3], 1)
round(1-cor(df.scaled["New Mexico",],df.scaled["Iowa",]),1)
round(1-cor(df.scaled["New Mexico",],df.scaled["Indiana",]),1)
round(1-cor(df.scaled["Iowa",],df.scaled["Indiana",]),1)

```


# Visualizing distance matrices
* A simple solution for visualizing the distance matrices is to use the function fviz_dist() [factoextra package].  
* Other specialized methods will be  described later on.

```{r}
library(factoextra)
fviz_dist(dist.eucl)
```

# Partitioning Clustering

* Partitioning clustering are clustering methods used to classify observations within a data set, into multiple groups based on their similarity.
* The algorithms require the analyst to specify the number of clusters to be generated.
*  This chapter describes the commonly used partitioning clustering, including:
1. K-means clustering (MacQueen, 1967), in which, each cluster is represented by the center or means of the data points belonging to the cluster. The K-means method is sensitive to anomalous data points and outliers.
2. K-medoids clustering or PAM (Partitioning Around Medoids, Kaufman & Rousseeuw, 1990), in which, each cluster is represented by one of the objects in the cluster. PAM is less sensitive to outliers compared to k-means.
3. CLARA algorithm (Clustering Large Applications), which is an extension to PAM adapted for large data sets. 



# K-Means Clustering

* The description of the algorithm is based on:
* HARTIGAN, John A. _Clustering algorithms_. John Wiley & Sons, Inc., 1975.
* The data used by the author are provided below.
 ![](HartiganTable4dot1.jpg)
* The principal nutrients in meat, fish, and fowl are listed.
* Recall that 1oz= 28.34952g. 
* Estimated daily dietary allowances are: food energy (3200 
cal), protein (70 g), calcium (0.8 g), and iron (10 mg).
* Table 4.2 convents the variables (with the exception of Fat) in percentage of food delivery.
![](HartiganTable4dot2.jpg) 
* For e.g., the first (BB) ligne is obtained in the following way: 
* $340/3200=11\%\text{(Food Energy)}$. 
* $20/70=29\%\text{(Protein)}$.
* ${0.009}/{0.8}=1\%\text{ (Calcium)}$.
* ${2.6}/{10}= 26\%\text{ (Iron)}$.
* An argument could be made that iron is less important than calories or protein and so should be given less weight or ignored entirely.
* There are $n$ objects and $k$ clusters, $k\leq n$.
* Our purpose is to partition the $n$ objects (here  foods) so that objects within clusters are close and objects in different clusters are distant.
* Each cluster contains at least one object and each object belongs to only one cluster. 
* There is a very large number of possible partitions.

# Exercice 9
What is the number of possible partitions ?

# K-Means
*  The discordance between the data and a given partition is denoted by $e$.
* We use the technique of local optimization.
* A neighborhood of partitions is defined for each ption.
* Starting from an initial partition, search through a set of partitions at each step.
* Move from the partition to a partition in its neighborhood for which $e$ is minim.
* If the neighborhoods are very large, it is cheaper computationally to move to the first partition discovered in the neighborhood where $e$ is reduced from its present value.
* A number of stopping rules are possible.
* For example, the search stops when $e$ is not reduced by movement to the neighborhood.
* The present partition is nidered locally optimal in that it is the best partition in its neighborhood.
* Consider partitions of the five  ($n=5$) beef foods $\{\text{BB, BR,BS,BC, into three clusters ($k=3$).
* Totally, there are 25 such partiermutations.
* A plausible neighborhood for a partition is the set of partitions obtained by transferring an object from one cluster to another.

* For the partition (BB BR) (BS) (BC BH), the neighborhood consists of the following ten partitions:

1. (BR) (BB BS) (BC BH)
2. (BR) (BS) (BB BC BH) 
3. (BB) (BR BS) (BC BH) 
4. (BB) (BS) (BR BC BH) 
5. (BB BR BS) O (BC BH) 
6. (BB BR) O (BS BC BH) 
7. (BB BR BC) (BS) (BH)
8. (BB BR) (BS BC) (BH) 
9. (BB BR BH) (BS) (BC) 
10. (BB BR) (BS BH) (BC)



# K-Means Algorithm
* Let $\mathbf{x}^j\equiv (x_1^j,\cdots,x_n^j)$ the vector of  values for the object $j$, $j=1,\cdots ,m.$
* The variables are assumed scaled.
* The partition has   $k$ disjoint clusters  clusters $C_1,\cdots,C_k$, which are the indices of the objects in the various clusters.
* Let $m_l$ be the number of objects in cluster $C_l$.
* Each of the $m$ objects lies in just one of the $k$ clusters.
* Note that $\sum_{l=1}^k m_l=m$.
* The vector of means over the objects in cluster $C_l$ is denoted by $\bar{\mathbf{x}}^{l}$, with
$$\bar{\mathbf{x}}^{l}\equiv\frac{1}{m_l}\sum_{j\in C_l}\mathbf{x}^{j}=(\bar{x}_1^l,\cdots,\bar{x}_n^l),\:l=1,\cdots, k,$$
where 
$$
\bar{x}_i^l\equiv \frac{\sum_{j\in C_l}x_i^{j}}{m_l},\:i=1,\cdots, n; \:l=1,\cdots,k.
$$
* The distance between the object $j$ and the cluster $l$ is $d(\mathbf{x}^j,\bar{\mathbf{x}}^l)$, where $d$ is taken to be the Euclidian distance
$$d(\mathbf{x}^j,\bar{\mathbf{x}}^l)=||\mathbf{x}^j-\bar{\mathbf{x}}^l||=\bigg[\sum_{i=1}^n(x_i^j-\bar{x}_i^l)^2\bigg]^{1/2},\:j=1,\cdots,m;\:l=1,\cdots,k.$$ 
where $||\mathbf{\cdot}||$ is the Euclidian norm.
*  The error of the partition is measuredtaken to bye  
$$e=\sum_{l=1}^{k}\sum_{j\in C_l}d^2(\mathbf{x}^j,\bar{\mathbf{x}}^l)=
\sum_{l=1}^{k}\sum_{j\in C_l}
||\mathbf{x}^j-\bar{\mathbf{x}}^l||^2.
$$
* Alternatively, we have
$$e=\sum_{j=1}^{m}d^2(\mathbf{x}^j,\bar{\mathbf{x}}^{l(j)})=\sum_{j=1}^{m}\sum_{i=1}^n||\mathbf{x}^j-\bar{\mathbf{x}}^{l(j)}||^2,$$

where $l(j)$ is the index of the cluster of object $j$.

* The general procedure is to search for a partition with a small error $e$ by moving cases from one cluster to another.
* The search ends when no such movement reduces $e$.
* STEP 1. Assume initial clusters. Compute the cluster means $\bar{\mathbf{x}}^l$ and the initial error $e$.
* STEP 2. For the first Object, compute for every cluster $l$ 
$$
\Delta e=
\frac{m_ld^2(\mathbf{x}^1,\bar{\mathbf{x}}^l)}{m_l+1}-\frac{m_{l(1)}d^2(\mathbf{x}^1,\bar{\mathbf{x}}^{l(1)})}{m_{l(1)}-1},\:l=1,\cdots, k,\:l\neq l(1).$$

* It corresponds to the error variation in transferring the first object from cluster $l(1)$ to which it belongs to cluster $l$.
* If the minimum of this quantity over all $l\neq l(1)$ is negative, transfer the first case from cluster $l(1)$ to this minimal $l$.
* Adjust the cluster means of $l(1)$ and the minimal $l$ and add the error variation (which is negative) to $e$.
* STEP 3. Repeat Step 2 for each object $j$ such that $2\leq j \leq m$.
* STEP 4. lf no movement of an object from one cluster to another occurs for any case, stop. Otherwise, return to Step 2.

# Exercice 10
Prove that the error variation is indeed given by: $$
\Delta e=
\frac{m_ld^2(\mathbf{x}^1,\bar{\mathbf{x}}^l)}{m_l+1}-\frac{m_{l(1)}d^2(\mathbf{x}^1,\bar{\mathbf{x}}^{l(1)})}{m_{l(1)}-1},\:l=1,\cdots, k,\:l\neq l(1). $$


# K-MEANS APPLIED TO FOOD NUTRIENT DATA
* Only the first eight foods will be considered.
* Only three variables, food energy, protein, and calcium as a percentage of recommended daily allowances are used.
* The eight foods $(m=8)$ are partitioned into three clusters ($k=3$).
  
 ![](HartiganTable4dot3.jpg)




<<<<<<< HEAD
```{r}
#library("cluster.datasets")
#write.csv(rda.meat.fish.fowl.1959,"Hartigandat%a1.csv")
df<-read.csv("Hartigandata1.csv")
print(df)
df<-df[1:8,c(3,4,6)]
df

```

```{r}
# The data contain some errors 
df[3,1]<-13 # Error in ligne 3
df[6,1]<-4 # Errhttps://raw.githubusercontent.com/karkil2205/Usid13Chapter1/master/Table35.jpg)
*Data from Everitt, B. S., Landau, S., Leese, M., & Stahl, D. (2011). Cluster Analysis. –John Wiley & Sons. Ltd., New York, 330.*

* The records of 11 students including one that had not yet experienced video games (subject 82) are shown in Table 3.5. 
* The dissimilarity between subjects 1 and 2 can be calculated as follows

$$\frac{0+|2-0|/30+|2-2|/3+0+|2-3|/3+1+0}{7}=0.2.$$

* The variables *like* and *freq* have been treated as or din ligne 6
df[7,3]<-1 # Error at ligne 7
df
rownames(df)<-c("BB","HR","BR","BS","BC","CB","CC","BH")
df
colnames(df)<-c("Energy","Protein","Calcium")
df
```

* STEP 1. An initial clustering which  works well is based on the object sums.
* We denote by $s_j$ the onject sum of object $j,\:j=1,\cdots,m$.al variables, as if their ranks were on an interval scale.
*  This is not ideal but seemed preferable to treating them as nominal variables.

```{r}
data=read.csv("https://raw.githubusercontent.com/karkil2205/Usid13Chapter1/master/videogames.csv")
data[1,]
data[2,]
```
measured characters.
* *The presence of the character or type is coded $1$ and the absence by 0. 
* We have have at our disposal two vectors.
* $\mathbf{x}$ is observed for first indiviual (or object).
* $\mathbf{y}$ is observed for a second individual. 
* We setcan the object $j$ to the cluser $l(j)$ such that 

$$
l(j)= \bigg[k\left(\frac{s_j-\min_{l=1\cdots k}s_l}{\max_{l=1\cdots k}s_l-\min_{l=1\cdots k}s_l}\right)+1\bigg],
$$

where $[\cdot]$ is tn calculate the following statistics:
 
   $a=\mathbf{x\cdot y}=\sum_{i=1}^nx_iy_i.$
   
   $b=\mathbf{(1-x)\cdot y}=\sum_{i=1}^n(1-x_i)y_i.$
   
   $c=\mathbf{x\cdot (1-y)}=\sum_{i=1}^nx_i(1-y_i).$

   $d=\mathbf{(1-x)\cdot (1-y)}=\sum_{i=1}^n(1-x_i)(1-y_i).$

* This gives a familiar $2 \times 2$ association table . 
* The ienteger part function.

```{r}
s=rowSums(df)
cluster=trunc(3*(s-min(s))/(max(s)-min(s))+1)
cluster
Myfunc<-function(x){m=min(x,3)
return(m)}
cluster<-sapply(cluster, Myfunc)
cluster
```

* Note that objets whith minimal sum are attributed to cluster $k+1$ with this rule and must be attributed to $k$.  
* Thus the initial partition shoul be: (BR CB) (HR BS BC) (BB CC BH).
```{r}
rownames(df)[cluster==1]  
rownames(df)[cluster==2]
rownames(df)[cluster==3]
```




* Hartigan has some errors since he found sums equal to $41,39,35,4ries are the counts of matches , $a$ for $(1,1)$,  and $d$ for $(0,0)$.
* The counts of mismatches are for $b$ for $(1,0)$ and $c$ for $(0,1)$).
* Obviously, $a+b+c+d=n$.

|    |    |Second individual|||
|:---|---|:---:|:---:|:---:|
|  |    |1  |0|*Totals*
|**First individual**|1|$a$|$b$|$a+b$|
|             |0|$c$|$d$|$c+d$|
|*Totals*||$a+c$|$b+d$|$n$|




* We will use data taken from: Kaufman, L., & Rousseeuw, P. J. (2009). _Finding groups in data: an introduction to cluster analysis_ (Vol. 344). John Wiley & Sons.

![KAUFMANBinarydata](KAUFMANBinarydata.jpg)

* For $8$ people, $10$ binary variables were considered:
* male/female, blue eyes/brown eyes, round face/oval face, and so on.
* The attribute listed first is always the one coded as 1, for instance blue eyes = 1 and brown eyes = 0.
* When comparing Ilan with Talia.
* We make up a table like (16) which yields a = 1, b = 3, c = 1, and d = 5.
* Note that interchanging Ilan and Talia would permute b and c (while leaving a and d unchanged), so a good similarity or dissimilarity coefficient must treat b and c in the same way in order to satisfy (D3) or (S3). At this point a cruc 



* A list of some of the similarity measures that have been suggested for binary data is shown in Table 3.3; a more extensive list can be found in Gower and Legendre (1986). 

data=c(
"Ilan",1,0,1,1,0,0,1,0,0,0,
"Jacqueline",0,1,0,0,1,0,0,0,0,0,
"Kim",0,0,1,0,0,0,1,0,0,1,
"Lieve",0,1,0,0,0,0,0,1,1,0,
"Leon",1,1,0,0,41,34,42,44$.

```{r}
s=c(41,39,35,40,41,34,42,44)
3*(s-min(s))/(max(s)-min(s))+1
cluster<-trunc(3*(s-min(s))/(max(s)-min(s))+1)
cluster
cluster<-sapply(cluster,Myfunc)
cluster
```1,0,1,1,0,
"Peter",1,1,0,0,1,0,1,1,0,0,
"Talia",0,0,0,1,0,1,0,0,0,0,
"Tina",0,0,0,1,0,1,0,0,0,0
)
data.frame(matrix(data, nrow=8,byrow=T))




|Measure|Formula|
|---|---|
|Matching coefficient|$(a+d)/(a+b+c+d)$|
|Jaccard coefficient|$a/(a+b+c)$|
|Rogers and Tanimoto (1960)| $(a+d)/[a+2(b+c)+d]$|
|Gower and Legendre (1986)|$(a+d)/[a+\frac{1}{2}(b+c)+d]$|
|Gower and Legendre (1986)|$a/[a+\frac{1}{2}(b+c)]$|



* The corresponding clusters are 3, 2, 1, 2, 3, 1, 3, and 3.
* Thus the initial partition is (BR CB) (HR BS) (BB BC CC BH). 

```{r}
rownamreason for such a large number of possible measures has to do with the apparent uncertainty as to how to deal with the count of zero–zero matches (df)[cluster==1]
rownames(df)[cluster==2]
rownames(df)[cluster==3]
```

* We then compute the means within each cluster.

```{r}
colMeans(df[cluster==1,])
colMeans(df[cluster==2,])
colMeans(df[cluster==3,])

```


* For example, the mean of objects in Table 3.2).
* In some cases, of course, zero–zero matches are completely equivalent to one–one matches, and therefore should be included in the calculated similarity measure.
* An example is gender, where there is no preference as to which of the two categories should be coded zero or one.
* But in othe first cluster fr cases the inclusion or othe first variable, equals (13 + 4)/2 = 8.5 (See Table 4.3 for more.)
* Trwise of d is more problematic; for example, when the zerror for the initial partition is the sum of squared distances of cases from their cluster means.


```{r}
MatrixC1=rbind(df[cluster==1,],colMeans(df[cluster==1,]))
#rownames(MatrixC1)[3]<-c("MeanC1")
#MatrixC1
#sum((as.matrix(D)[3,])^2)
#MatrixC2=rbind(df[cluster==2,],colMeans(df[cluster==2,]))
#MatrixC2
#rownames(MatrixC2)[3]<-c("MeanC2")
#MatrixC3=rbind(df[cluster==3,],colMeans(df[cluster==3,]))
#rownames(MatrixC3)[5]<-c("MeanC3")
#MatrixC3

```


<<<<<<< HEAD
=======


=======
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTIxNTc1NjIxMSwyMDc3NTU5ODYsLTEzOD
A4NDYyMzAsODA0MzYxMTg5LC0xMjcwODIwMjgyLDYwMzcwMzM5
MiwxNzc4NTQwMjA2LC0xOTUzNjQ3NTgwLC0xMjEzMzgyMTczLD
E2OTAwNjQ2NDEsLTEwMjIxOTM2MzAsMjEzOTc1NzM4MCwtMjI0
MjI1OTIxLC0xODczMzk0NjI5LC05MDE4MzY2MzEsLTE3MzU5OD
g5ODAsMjA3MzAzMTU4MiwtNzE3OTkxOTY2LC0xNzExMTAyNzYs
Mjg5MzI4MDMyXX0=
-->
>>>>>>> 3caf7d63d6017c0a7714fed0a79edcda89457222
<!--stackedit_data:
eyJoaXN0b3J5IjpbNDMyMzU0ODMyXX0=
-->
>>>>>>> 8caa4cc47f58295532ed8578645f749b91ff0c7fo category corresponds to the genuine absence of some property, such as wings in a study of insects. 


# Distance matrix computation
* We’ll use a subset of the data USArrests
*  We’ll use only a by taking 15 random rows among the 50 rows in the data set. 
* Next, we standardize the data using the function scale():

```{r}
install.packages("FactoMineR")
library("FactoMineR")
data("USArrests") # Loading
head(USArrests, 3) # Print the first 3 rows
set.seed(123)
ss <- sample(1:50, 15) # Take 15 random rows
df <- USArrests[ss, ] # Subset the 15 rows
df.scaled <- scale(df) # Standardize the variables
```


<!--stackedit_data:
eyJoaXN0b3J5IjpbMTM1NzE3NTczOF19
-->