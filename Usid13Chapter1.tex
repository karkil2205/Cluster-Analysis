% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Distance and dissimilarities},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

\title{Distance and dissimilarities}
\author{}
\date{\vspace{-2.5em}}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{2}
\tableofcontents
}
\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\OperatorTok{::}\NormalTok{opts_chunk}\OperatorTok{$}\KeywordTok{set}\NormalTok{(}\DataTypeTok{echo =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{definition-of-a-distance}{%
\section{Definition of a distance}\label{definition-of-a-distance}}

\begin{itemize}
\item
  A distance function or a metric on \(\mathbb{R}^n,\:n\geq 1\), is a
  function \(d:\mathbb{R}^n\times\mathbb{R}^n\rightarrow \mathbb{R}\).
\item
  A distance function must satisfy some required properties or axioms.
\item
  There are three main axioms.
\item
  A1. \(d(\mathbf{x},\mathbf{y})= 0\iff \mathbf{x}=\mathbf{y}\)
  (identity of indiscernibles);
\item
  A2. \(d(\mathbf{x},\mathbf{y})= d(\mathbf{y},\mathbf{x})\) (symmetry);
\item
  A3.
  \(d(\mathbf{x},\mathbf{z})\leq d(\mathbf{x},\mathbf{y})+d(\mathbf{y},\mathbf{z})\)
  (triangle inequality), where \(\mathbf{x}=(x_1,\cdots,x_n)\),
  \(\mathbf{y}=(y_1,\cdots,y_n)\) and \(\mathbf{z}=(z_1,\cdots,z_n)\)
  are all vectors of \(\mathbb{R}^n\).
\item
  We should use the term \emph{dissimilarity} rather than
  \emph{distance} when not all the three axioms A1-A3 are valid.
\item
  Most of the time, we shall use, with some abuse of vocabulary, the
  term distance.
\end{itemize}

\hypertarget{exercice-1}{%
\section{Exercice 1}\label{exercice-1}}

\begin{itemize}
\tightlist
\item
  Prove that the three axioms A1-A3 imply the non-negativity condition:
  \[d(\mathbf{x},\mathbf{y})\geq 0.\]
\end{itemize}

\hypertarget{euclidean-distance}{%
\section{Euclidean distance}\label{euclidean-distance}}

\begin{itemize}
\tightlist
\item
  It is defined by:
\end{itemize}

\[d(\mathbf{x},\mathbf{y})=\sqrt{\sum_{i=1}^n (x_i-y_i)^2}.\] * A1A2 ae
onbvious. * The proof of A3 is provided below.

\hypertarget{manhattan-distance}{%
\section{Manhattan distance}\label{manhattan-distance}}

\begin{itemize}
\tightlist
\item
  The Manhattan distance also called taxi-cab metric or city-block
  metric is defined by:
\end{itemize}

\[d(\mathbf{x},\mathbf{y})
=\sum_{i=1}^n |x_i-y_i|.
\]

\begin{itemize}
\tightlist
\item
  A1-A2 hold.
\item
  A3 also holds using the fact that \(|a+b|\leq |a|+|b|\) for any reals
  \(a,b\).
\item
  There exists also a weighted version of the Manhattan distance called
  the Canberra distance.
\end{itemize}

\href{https://upload.wikimedia.org/wikipedia/commons/0/08/Manhattan_distance.svg}{Manhattan
distance vs Euclidean distance Graph}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{y =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{)}
\KeywordTok{dist}\NormalTok{(}\KeywordTok{rbind}\NormalTok{(x, y), }\DataTypeTok{method =} \StringTok{"euclidian"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          x
## y 8.485281
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{6}\OperatorTok{*}\KeywordTok{sqrt}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 8.485281
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dist}\NormalTok{(}\KeywordTok{rbind}\NormalTok{(x, y), }\DataTypeTok{method =} \StringTok{"manhattan"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##    x
## y 12
\end{verbatim}

\hypertarget{canberra-distance}{%
\section{Canberra distance}\label{canberra-distance}}

\begin{itemize}
\tightlist
\item
  It is defined by:
\end{itemize}

\[d(\mathbf{x},\mathbf{y})
=\sum_{i=1}^n \frac{|x_i-y_i|}{|x_i|+|y_i|}.\]

\begin{itemize}
\tightlist
\item
  Note that the term \(|x_i − y_i|/(|x_i|+|y_i|)\) is not properly
  defined when \(x_i=y_i=0\).
\item
  By convention we set the ratio to be zero in that case.
\item
  The Canberra distance is specially sensitive to small changes near
  zero.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{y =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{)}
\KeywordTok{dist}\NormalTok{(}\KeywordTok{rbind}\NormalTok{(x, y), }\DataTypeTok{method =} \StringTok{"canberra"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   x
## y 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{6}\OperatorTok{/}\DecValTok{6}\OperatorTok{+}\DecValTok{6}\OperatorTok{/}\DecValTok{6}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2
\end{verbatim}

\hypertarget{exercice-2}{%
\section{Exercice 2}\label{exercice-2}}

\begin{itemize}
\tightlist
\item
  Prove that the Canberra distance is a true distance.
\end{itemize}

\hypertarget{minkowski-distance}{%
\section{Minkowski distance}\label{minkowski-distance}}

\begin{itemize}
\tightlist
\item
  Both the Euclidian and the Manattan distances are special cases of the
  Minkowski distance which is defined, for \(p\geq 1\), by: \[
  d(\mathbf{x},\mathbf{y})=
  \left[\sum_{i=1} |x_i-y_i|^{p}\right]^{1/p}.
  \]
\item
  For \(p=1\), we get the Manhattan distance.
\item
  For \(p=2\), we get the Euclidian distance.
\item
  Let us also define:
  \[\|\mathbf{x}\|_p\equiv\left[\sum_{i=1}^n |x_i|^{p}\right]^{1/p},\]
  where \(\|\mathbf{\cdot}\|_p\) is known as the \(p\)-norm or Minkowski
  norm.
\item
  Note that the Minkowski distance and norm are related by: \[
  d(\mathbf{x},\mathbf{y})=\|\mathbf{x}-\mathbf{y}\|_p.
  \]
\item
  Conversely, we have: \[
  \|\mathbf{x}\|_p=d(\mathbf{x},\mathbf{0}),
  \] where \(\mathbf{0}\) is the null-vetor of \(\mathbb{R}^n\).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"ggplot2"}\NormalTok{)}
\NormalTok{x =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{)}
\NormalTok{y =}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{,}\DecValTok{6}\NormalTok{)}
\NormalTok{MinkowDist=}\KeywordTok{c}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (p }\ControlFlowTok{in} \KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{30}\NormalTok{,.}\DecValTok{01}\NormalTok{))}
\NormalTok{\{}
\NormalTok{MinkowDist=}\KeywordTok{c}\NormalTok{(MinkowDist,}\KeywordTok{dist}\NormalTok{(}\KeywordTok{rbind}\NormalTok{(x, y), }\DataTypeTok{method =} \StringTok{"minkowski"}\NormalTok{, }\DataTypeTok{p =}\NormalTok{ p))     }
\NormalTok{\}}
\KeywordTok{ggplot}\NormalTok{(}\DataTypeTok{data =}\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{30}\NormalTok{,.}\DecValTok{01}\NormalTok{), }\DataTypeTok{y=}\NormalTok{MinkowDist ) , }\DataTypeTok{mapping =} \KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }\DataTypeTok{y =}\NormalTok{ y))}\OperatorTok{+}\KeywordTok{geom_point}\NormalTok{(}\DataTypeTok{size=}\NormalTok{.}\DecValTok{1}\NormalTok{,}\DataTypeTok{color=}\StringTok{"red"}\NormalTok{)}\OperatorTok{+}\KeywordTok{xlim}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{11}\NormalTok{)}\OperatorTok{+}\KeywordTok{xlab}\NormalTok{(}\StringTok{"p"}\NormalTok{)}\OperatorTok{+}\KeywordTok{ylab}\NormalTok{(}\StringTok{"Minkowski Distance"}\NormalTok{)}\OperatorTok{+}\KeywordTok{ggtitle}\NormalTok{(}\StringTok{"Minkowski distance wrt p"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: Removed 1900 rows containing missing values (geom_point).
\end{verbatim}

\includegraphics{Usid13Chapter1_files/figure-latex/unnamed-chunk-3-1.pdf}

\hypertarget{chebyshev-distance}{%
\section{Chebyshev distance}\label{chebyshev-distance}}

\begin{itemize}
\tightlist
\item
  At the limit, we get the Chebyshev distance which is defined by: \[
  d(\mathbf{x},\mathbf{y})=\max_{i=1,\cdots,n}(|x_i-y_i|)=\lim_{p\rightarrow\infty}
  \left[\sum_{i=1} |x_i-y_i|^{p}\right]^{1/p}.
  \]
\item
  The corresponding norm is: \[
  \|\mathbf{x}|_\infty=\max_{i=1,\cdots,n}(|x_i|).
  \]
\end{itemize}

\hypertarget{minkowski-inequality}{%
\section{Minkowski inequality}\label{minkowski-inequality}}

\begin{itemize}
\item
  The proof of the triangular inequality A3 is based on the Minkowski
  inequality:
\item
  For any nonnegative real numbers \(a_1,\cdots,a_n\);
  \(b_1,\cdots,b_n\), and for any \(p\geq 1\), we have: \[
  \left[\sum_{i=1}^n (a_i+b_i)^{p}\right]^{1/p}\leq
  \left[\sum_{i=1}^n a_i^{p}\right]^{1/p}
  +\left[\sum_{i=1}^n b_i^{p}\right]^{1/p}.
  \]
\item
  To prove that the Minkowski distance satisfies A3, notice that \[
   \sum_{i=1}^n|x_i-z_i|^{p}= \sum_{i=1}^n|(x_i-y_i)+(y_i-z_i)|^{p}.
  \]
\item
  Since for any reals \(x,y\), we have: \(|x+y|\leq |x|+|y|\), and using
  the fact that \(x^p\) is increasing in \(x\geq 0\), we obtain: \[
   \sum_{i=1}^n|x_i-z_i|^{p}\leq \sum_{i=1}^n(|x_i-y_i|+|y_i-z_i|)^{p}.
  \]
\item
  Applying the Minkowski inequality with \(a_i=|x_i-y_i|\) and
  \(b_i=|y_i-z_i|\), \(i=1,\cdots,n\), we get: \[
   \sum_{i=1}^n|x_i-z_i|^{p}\leq \left(\sum_{i=1}^n |x_i-y_i|^{p}\right)^{1/p}+\left(\sum_{i=1}^n |y_i-z_i|^{p}\right)^{1/p}.
  \]
\end{itemize}

\hypertarget{huxf6lder-inequality}{%
\section{Hölder inequality}\label{huxf6lder-inequality}}

\begin{itemize}
\tightlist
\item
  The proof of the Minkowski inequality itself requires the Hölder
  inequality:
\item
  For any nonnegative real numbers \(a_1,\cdots,a_n\);
  \(b_1,\cdots,b_n\), and any \(p,q>1\) with \(1/p+1/q=1\), we have: \[
  \sum_{i=1}^n a_ib_i\leq
  \left[\sum_{i=1}^n a_i^{p}\right]^{1/p}
  \left[\sum_{i=1}^n b_i^{q}\right]^{1/q}
  \]
\item
  The proof of the Hölder inequality relies on the Young inequality:
\item
  For any \(a,b>0\), we have \[
  ab\leq \frac{a^p}{p}+\frac{b^q}{q},
  \] with equality occuring iff: \(a^p=b^q\).
\item
  To prove the Young inequality, one can use the (strict) convexity of
  the exponential function.
\item
  For any reals \(x,y\), we have: \[
  e^{\frac{x}{p}+\frac{y}{q} }\leq \frac{e^{x}}{p}+\frac{e^{y}}{q}. 
  \]
\item
  We then set: \(x=p\ln a\) and \(y=q\ln b\) to get the Young
  inequality.
\item
  A good reference on inequalities is: Z. Cvetkovski, Inequalities:
  theorems, techniques and selected problems, 2012, Springer Science \&
  Business Media. \# Cauchy-Schwartz inequality
\item
  Note that the triangular inequality for the Minkowski distance
  implies: \[
  \sum_{i=1}^n |x_i|\leq
  \left[\sum_{i=1}^n |x_i|^{p}\right]^{1/p}.
  \]
\item
  Note that for \(p=2\), we have \(q=2\). The Hölder inequality implies
  for that special case \[
  \sum_{i=1}^n|x_iy_i|\leq\sqrt{\sum_{i=1}^n x_i^2}\sqrt{\sum_{i=1}^n y_i^2}. 
  \]
\item
  Since the LHS od thes above inequality is greater then
  \(|\sum_{i=1}^nx_iy_i|\), we get the Cauchy-Schwartz inequality
\end{itemize}

\[
|\sum_{i=1}^nx_iy_i|\leq\sqrt{\sum_{i=1}^n x_i^2}\sqrt{\sum_{i=1}^n y_i^2}. 
\] * Using the dot product notation called also scalar product noation:
\(\mathbf{x\cdot y}=\sum_{i=1}^nx_iy_i\), and the norm notation
\(\|\mathbf{\cdot}\|_2 \|\), the Cauchy-Schwart inequality is: \[
|\mathbf{x\cdot y} | \leq \|\mathbf{x}\|_2 \| \mathbf{y}\|_2.
\]

\hypertarget{pearson-correlation-distance}{%
\section{Pearson correlation
distance}\label{pearson-correlation-distance}}

\begin{itemize}
\item
  The Pearson correlation coefficient is a similarity measure on
  \(\mathbb{R}^n\) defined by: \[
  \rho(\mathbf{x},\mathbf{y})=
  \frac{\sum_{i=1}^n (x_i-\bar{\mathbf{x}})(y_i-\bar{\mathbf{y}})}{{\sqrt{\sum_{i=1}^n (x_i-\bar{\mathbf{x}})^2\sum_{i=1}^n (y_i-\bar{\mathbf{y}})^2}}},
  \] where \(\bar{\mathbf{x}}\) is the mean of the vector \(\mathbf{x}\)
  defined by: \[\bar{\mathbf{x}}=\frac{1}{n}\sum_{i=1}^n x_i,\]
\item
  Note that the Pearson correlation coefficient satisfies P2 and is
  invariant to any positive linear transformation, i.e.:
  \[\rho(\alpha\mathbf{x},\mathbf{y})=\rho(\mathbf{x},\mathbf{y}),\] for
  any \(\alpha>0\).
\item
  The Pearson distance (or correlation distance) is defined by: \[
  d(\mathbf{x},\mathbf{y})=1-\rho(\mathbf{x},\mathbf{y}).\]
\item
  Note that the Pearson distance does not satisfy A1 since
  \(d(\mathbf{x},\mathbf{x})=0\) for any non-zero vector \(\mathbf{x}\).
  It neither satisfies the triangle inequality. However, the symmetry
  property is fullfilled.
\end{itemize}

\hypertarget{cosine-correlation-distance}{%
\section{Cosine correlation
distance}\label{cosine-correlation-distance}}

\begin{itemize}
\tightlist
\item
  The cosine of the angle \(\theta\) between two vectors \(\mathbf{x}\)
  and \(\mathbf{y}\) is a measure of similarity given by: \[
  \cos(\theta)=\frac{\mathbf{x}\cdot \mathbf{y}}{\|\mathbf{x}\|_2\|\mathbf{y}\|_2}=\frac{\sum_{i=1}^n x_i y_i}{{\sqrt{\sum_{i=1}^n x_i^2\sum_{i=1}^n y_i^2}}}.
  \]
\item
  Note that the cosine of the angle between the two centred vectors
  \((x_1-\bar{\mathbf{x}},\cdots,x_n-\bar{\mathbf{x}})\) and
  \((y_1-\bar{\mathbf{y}},\cdots,y_n-\bar{\mathbf{y}})\) coincides with
  the Pearson correlation coefficient of \(\mathbf{x}\) and
  \(\mathbf{y}\).\\
\item
  The cosine correlation distance is defined by: \[
  d(\mathbf{x},\mathbf{y})=1-\cos(\theta).
  \]
\item
  It shares similar properties than the Pearson correlation distance.
  Likewise, Axioms A1 and A3 are not satisfied.
\end{itemize}

\hypertarget{spearman-correlation-distance}{%
\section{Spearman correlation
distance}\label{spearman-correlation-distance}}

\begin{itemize}
\tightlist
\item
  To calculate the Spearman's rank-order correlation, we need to map
  seperately each of the vectors to ranked data values:
  \[\mathbf{x}\rightarrow \text{rank}(\mathbf{x})=(x_1^r,\cdots,x_n^r).\]
\item
  Here, \(x_i^r\) is the rank of \(x_i\) among the set of values of
  \(\mathbf{x}\).
\item
  We illustrate this transformation with a simple example:
\item
  If \(\mathbf{x}=(3, 1, 4, 15, 92)\), then the rank-order vector is
  \(\text{rank}(\mathbf{x})=(2,1,3,4,5)\).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x=}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{92}\NormalTok{)}
\KeywordTok{rank}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2 1 3 4 5
\end{verbatim}

\begin{itemize}
\tightlist
\item
  The Spearman's rank correlation of two numerical variables
  \(\mathbf{x}\) and \(\mathbf{y}\) is simply the Pearson correlation of
  the two correspnding rank-order variables \(\text{rank}(\mathbf{x})\)
  and \(\text{rank}(\mathbf{y})\),
  i.e.~\(\rho(\text{rank}(\mathbf{x}),\text{rank}(\mathbf{y}))\). This
  measure is is useful because it is more robust against outliers than
  the Pearson correlation.
\item
  If all the \(n\) ranks are distinct, it can be computed using the
  following formula: \[
  \rho(\text{rank}(\mathbf{x}),\text{rank}(\mathbf{y}))=1-\frac{6\sum_{i=1}^n d_i^2}{n(n^2-1)},
  \] where \(d_i=x_i^r-y_i^r,\:i=1,\cdots,n\).
\item
  The spearman distance is then defined by: \[
  d(\mathbf{x},\mathbf{y})=1-\rho(\text{rank}(\mathbf{x}),\text{rank}(\mathbf{y})).
  \]
\item
  It can be shown that easaly that it is not a proper distance.
\item
  If all the \(n\) ranks are distinct, we get: \[
  d(\mathbf{x},\mathbf{y})=\frac{6\sum_{i=1}^n d_i^2}{n(n^2-1)}.
  \]
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x=}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{92}\NormalTok{)}
\KeywordTok{rank}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2 1 3 4 5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y=}\KeywordTok{c}\NormalTok{(}\DecValTok{30}\NormalTok{,}\DecValTok{2}\NormalTok{ , }\DecValTok{9}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{48}\NormalTok{)}
\KeywordTok{rank}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4 1 2 3 5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d=}\KeywordTok{rank}\NormalTok{(x)}\OperatorTok{-}\KeywordTok{rank}\NormalTok{(y)}
\NormalTok{d}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -2  0  1  1  0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor}\NormalTok{(}\KeywordTok{rank}\NormalTok{(x),}\KeywordTok{rank}\NormalTok{(y))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1-6}\OperatorTok{*}\KeywordTok{sum}\NormalTok{(d}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\NormalTok{(}\DecValTok{5}\OperatorTok{*}\NormalTok{(}\DecValTok{5}\OperatorTok{^}\DecValTok{2-1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.7
\end{verbatim}

\hypertarget{kendall-tau-distance}{%
\section{Kendall tau distance}\label{kendall-tau-distance}}

\begin{itemize}
\tightlist
\item
  The Kendall rank correlation coefficient is calculated from the number
  of correspondances between the rankings of \(\mathbf{x}\) and the
  rankings of \(\mathbf{y}\).
\item
  The number of pairs of observations among \(n\) observations or values
  is: \[{n \choose 2} =\frac{n(n-1)}{2}.\]
\item
  The pairs of observations \((x_{i},x_{j})\) and \((y_{i},y_{j})\) are
  said to be \emph{concordant} if:
  \[\text{sign}(x_j-x_i)=\text{sign}(y_j-y_i),\] and to be
  \emph{discordant} if: \[\text{sign}(x_j-x_i)=-\text{sign}(y_j-y_i),\]
  where \(\text{sign}(\cdot)\) returns \(1\) for positive numbers and
  \(-1\) negative numbers and \(0\) otherwise.
\item
  If \(x_i=x_j\) or \(y_i=y_j\) (or both), there is a tie.
\item
  The Kendall \(\tau\) coefficient is defined by (neglecting ties):
  \[\tau =\frac {1}{n(n-1)}\sum_{i=1}^{n}\sum_{j=1}^n\text{sign}(x_j-x_i)\text{sign}(y_j-y_i).\]
\item
  Let \(n_c\) (resp. \(n_d\)) be the number of concordant (resp.
  discordant) pairs, we have \[\tau =\frac {2(n_c-n_d)}{n(n-1)}.\]
\item
  The Kendall tau distance is then:
  \[d(\mathbf{x},\mathbf{y})=1-\tau. \]
\item
  Remark: the triangular inequality may fail in cases where there are
  ties.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x=}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{92}\NormalTok{)}
\NormalTok{y=}\KeywordTok{c}\NormalTok{(}\DecValTok{30}\NormalTok{,}\DecValTok{2}\NormalTok{ , }\DecValTok{9}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{48}\NormalTok{)}
\NormalTok{tau=}\DecValTok{0}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{)}
\NormalTok{\{  }
\NormalTok{tau=tau}\OperatorTok{+}\KeywordTok{sign}\NormalTok{(x }\OperatorTok{-}\NormalTok{x[i])}\OperatorTok{%*%}\KeywordTok{sign}\NormalTok{(y }\OperatorTok{-}\NormalTok{y[i])}
\NormalTok{\}}
\NormalTok{tau=tau}\OperatorTok{/}\NormalTok{(}\DecValTok{5}\OperatorTok{*}\DecValTok{4}\NormalTok{)}
\NormalTok{tau}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      [,1]
## [1,]  0.6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{cor}\NormalTok{(x,y, }\DataTypeTok{method=}\StringTok{"kendall"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.6
\end{verbatim}

\hypertarget{variables-standardization}{%
\section{Variables standardization}\label{variables-standardization}}

\begin{itemize}
\tightlist
\item
  Variables are often standardized before measuring dissimilarities.
\item
  Standardization converts the original variables into uniteless
  variables.
\item
  A well known method is the z-score transformation: \[
  \mathbf{x}\rightarrow (\frac{x_1-\bar{\mathbf{x}}}{s_\mathbf{x}},\cdots,\frac{x_n-\bar{\mathbf{x}}}{s_\mathbf{x}}),
  \] where \(s_\mathbf{x}\) is the sample standard deviation given by:
  \[
  s_\mathbf{x}=\frac{1}{n-1}\sum_{i=1}^n(x_i-\bar{\mathbf{x}})^2.
  \]
\item
  The transformed variable will have a mean of \(0\) and a variance of
  \(1\).
\item
  The result obtained with Pearson correlation measures and standardized
  Euclidean distances are comparable.
\item
  For other methods, see: Milligan, G. W., \& Cooper, M. C. (1988). A
  study of standardization of variables in cluster analysis.
  \emph{Journal of classification}, \emph{5}(2), 181-204.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x=}\KeywordTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{92}\NormalTok{)}
\NormalTok{y=}\KeywordTok{c}\NormalTok{(}\DecValTok{30}\NormalTok{,}\DecValTok{2}\NormalTok{ , }\DecValTok{9}\NormalTok{, }\DecValTok{20}\NormalTok{, }\DecValTok{48}\NormalTok{)}
\NormalTok{(x}\OperatorTok{-}\KeywordTok{mean}\NormalTok{(x))}\OperatorTok{/}\KeywordTok{sd}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -0.5134116 -0.5647527 -0.4877410 -0.2053646  1.7712699
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{scale}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            [,1]
## [1,] -0.5134116
## [2,] -0.5647527
## [3,] -0.4877410
## [4,] -0.2053646
## [5,]  1.7712699
## attr(,"scaled:center")
## [1] 23
## attr(,"scaled:scale")
## [1] 38.9551
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(y}\OperatorTok{-}\KeywordTok{mean}\NormalTok{(y))}\OperatorTok{/}\KeywordTok{sd}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]  0.45263128 -1.09293895 -0.70654639 -0.09935809  1.44621214
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{scale}\NormalTok{(y)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             [,1]
## [1,]  0.45263128
## [2,] -1.09293895
## [3,] -0.70654639
## [4,] -0.09935809
## [5,]  1.44621214
## attr(,"scaled:center")
## [1] 21.8
## attr(,"scaled:scale")
## [1] 18.11629
\end{verbatim}

\hypertarget{distance-matrix-computation}{%
\section{Distance matrix
computation}\label{distance-matrix-computation}}

\begin{itemize}
\tightlist
\item
  We'll use a subset of the data USArrests
\item
  We'll use only a by taking 15 random rows among the 50 rows in the
  data set.
\item
  Next, we standardize the data using the function scale():
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"FactoMineR"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Installing package into '/home/rstudio-user/R/x86_64-pc-linux-gnu-library/4.0'
## (as 'lib' is unspecified)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"FactoMineR"}\NormalTok{)}
\KeywordTok{data}\NormalTok{(}\StringTok{"USArrests"}\NormalTok{) }\CommentTok{# Loading}
\KeywordTok{head}\NormalTok{(USArrests, }\DecValTok{3}\NormalTok{) }\CommentTok{# Print the first 3 rows}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         Murder Assault UrbanPop Rape
## Alabama   13.2     236       58 21.2
## Alaska    10.0     263       48 44.5
## Arizona    8.1     294       80 31.0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{ss <-}\StringTok{ }\KeywordTok{sample}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{50}\NormalTok{, }\DecValTok{15}\NormalTok{) }\CommentTok{# Take 15 random rows}
\NormalTok{df <-}\StringTok{ }\NormalTok{USArrests[ss, ] }\CommentTok{# Subset the 15 rows}
\NormalTok{df.scaled <-}\StringTok{ }\KeywordTok{scale}\NormalTok{(df) }\CommentTok{# Standardize the variables}
\end{Highlighting}
\end{Shaded}

\end{document}
